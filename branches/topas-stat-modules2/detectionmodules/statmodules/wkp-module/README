                   Wilcoxon-Mann-Whitney, Kolmogorov-Smirnov
                              & Pearson chi-square
                                      +
                                    Cusum
                                detection module
                   =========================================



To use this module, you have to:

1) enable it in the IPFIX collector's XML configuration file, through
   <module>
     <filename>path/to/wkp-module</filename>
     <run>yes</run>
     <arg>path/to/wkp-module-xml-configuration-file</arg>
   </module>

2) provide some configuration information in the module's XML
   configuration file; this information is detailled in section I of
   this README file



Table of contents
=================

I   - Configuration
II  - Some words about testing
III - How does it work?
IV  - Known bugs and limitations



I - Configuration
=================

The module's XML configuration file must provide the following three sections:
- <preferences>: user's preferences about the module
- <cusum-test-params>: parameters for the cusum test
- <wkp-test-params>: parameters for the statistical wkp-tests


-------------
<preferences>
-------------

<logfile>:

path to the module logfile. If not given, wkp_log.txt will be
used as default. If troubles opening it occur (e.g. problems with rights),
the module will issue an error message and exit.


<accepted_source_ids>:

comma-seperated list of source ids (of the exporting processes) to be
accepted. "all" is also possible. If parameter is omitted or left blank, all source ids will be accepted, too.


<alarm_time>:

time (in sec.) between two "test-runs". A "test-run" is defined as the
time when the module gathers information from the data-store, updates
its own samples information, and -- possibly, but not systematically
(see <stat_test_frequency>) -- runs Wilcoxon and/or Kolmogorov and/or
Pearson chi-square and(or cusum statistical tests. If no alarm time is given, a
warning message is issued and the DEFAULT_alarm_time will be used.
!!! NOTE: In OFFLINE MODE, this parameter will be ignored and 0 used instead.


<warning_verbosity>:

This parameter defines, which messages will be printed out by the wkp-/cusum-module.
Values can be 0, 1, 2, 3, 4 or 5, with the following meaning:
O: No messages are printed out
1: Fatal Errors
2: Same as 1 + normal Errors
3: Same as 2 + Warnings
4: Same as 3 + Informations
5: Same as 4 + Debug
If not given, a warning message is issued and default 3 is used; if
something else as 0,1,2,3,4 or 5 is given, an error message is issued and the
module exits.


<logfile_output_verbosity>:

O: no output generated
1: only p-values and attacks are recorded
2: plus some cosmetics
3: plus learning phases, updates and empty records events
4: plus sample printing
5: plus all details from statistical tests
If not given, a warning message is issued and default 3 is used; if
something less than 0 or greater than 5 is given, an error message is
issued and the module exits.
!!! WARNING: With many tests enabled and a high value for this parameter, the logfile quickly
reaches sizes up to several hundreds of MB!


<endpoint_key>: // FOR AGGREGATION

The user can choose between ip, port, protocol, any combination of them (seperated by spaces) OR "all" OR "none". This value will form the key of the EndPoints, i. e. that all EndPoints sharing this same key are considered as one EndPoint.
E. g. if you are only interested in ports and protocols, ip-addresses will be ignored and an EndPoint will look like "0.0.0.0:port|protocol". This enables the user to investigate port/protocol/ip-combination anomalies. If omitted, "none" will be used as default endpoint key, i. e. that there will be only one endpoint, 0.0.0.0:0|0, and a warning message is printed. "all" and "ip port protocol" yield the same result.


<netmask>: // FOR AGGREGATION

This parameter strongly relates toe the endpoint_key parameter. With this netmask, a value between 0 and 32, every ip address will be masked before further investigated. This allows you to aggregate ip addresses to subnets. If omitted, 32 will be used after a warning message, meaning that every ip address will be considered as it is. Please note, that this netmask is a global one for aggregating purposes and has nothing to do with the netmasks provided in the endpoints_to_monitor file! They are for filtering purposes!


<use_pca>:

If this parameter is set to "true", the tests will be performed on the principal components of the metrics instead of the metrics itself. If the parameter is omitted or contains any other value than "true", pca wont be performed.


<pca_learning_phase>:

If pca shall be performed (that means, use_pca parameter is set to "true), this parameter defines the length of the learning phase, i. e. how many values shall be considered for the calculation of the covariance matrix and its eigenvectors, the components. If omitted, DEFAULT_learning_phase_for_pca will be used.


<use_correlation_matrix>:

This parameter allows you to choose between the covariance matrix and their standardized form, the correlation matrix, used for calculation of the principal components. If set to "true", the correlation matrix will be used, if set to false or omitted, the covariance matrix will be used.


<metrics>:

The user should choose which metrics (s)he prefers to monitor and add each of them
per <value></value>-Tag. The possible values are:

<value>packets_in</value>
<value>packets_out</value>
<value>bytes_in</value>
<value>bytes_out</value>
<value>records_in</value>
<value>records_out</value>
<value>bytes_in/packet_in</value>
<value>bytes_out/packet_out</value>
<value>packets_in/record_in</value>
<value>packets_out/record_out</value>
<value>bytes_in/record_in</value>
<value>bytes_out/record_out</value>
<value>packets_out-packets_in</value>
<value>bytes_out-bytes_in</value>
<value>records_out-records_in</value>
<value>packets_in(t)-packets_in(t-1)</value>
<value>packets_out(t)-packets_out(t-1)</value>
<value>bytes_in(t)-bytes_in(t-1)</value>
<value>bytes_out(t)-bytes_out(t-1)</value>
<value>records_in(t)-records_in(t-1)</value>
<value>records_out(t)-records_out(t-1)</value>

Example configuration:

  <metrics>
    <value>packets_out</value>
    <value>bytes_in/packet_in</value>
  </metrics>

The input is not case-sensitive, and "bytes" and "octets" are synonymous.
If nothing or something else is given, the module exits after an
error message. Please note that the module does in fact NOT handle
octets/packet, but rather 1000*octets/packet, the result being an
integer: this trick enables to "simulate" a float division and
increase precision, without re-writing loads of existing code.
Another special case are the metrics 1000_times_records_in_per_packets_in etc.
They represent the metrics packets_in/record_in etc. and are needed, because in case of
an anomaly, the values of these metrics would decrease, but the
cusum-algorithm only considers anomalies with increasing metrics. Therefor, these
metrics will be calculated by records_in * 1000 / packets_in etc.


<noise_threshold_packets>, <noise_threshold_bytes>:

Do not consider the sample values below a certain threshold.
This options are motivated by the fact that sometimes, some IP addresses
have insignificant or irregular network traffic activity, sending only
a dozen or so packets from time to time, like "residual" traffic, "noise",
that should be cut off to avoid monitoring irregular and insignificant
network traffic. This helps reducing the number of "false positive" alarms.
Noise reduction is active only if considered metric contains packets or bytes; these
parameters will be ignored if metric is something else.
If no noise reduction prefererences are provided and if metric contains
packets or bytes, a warning message is issued and default values are used
(DEFAULT_noise_threshold_packets and DEFAULT_noise_threshold_bytes).


<endpointlist_maxsize>:

This parameter defines the number of maximal observable endpoints. If this
size is reached, every further endpoint will be ignored. This is to avoid
storage exhaustion, if you run the module for a longer time. The "bigger" the
endpoint_key, the more endpoints will be monitored, so if you e. g. use the
combination of ip/port/protocol as endpoint_key, you should think about increasing the maxsize of the endpointlist. If nothing is given, default value will be used (DEFAULT_endpointlist_maxsize).


<endpoints_to_monitor>: // FOR FILTERING

The user can specify a file containing all the endpoints he wants to be considered for the online tests, one endpoint per line. The format of the endpoints has to be:
ip.ip.ip.ip/netmask:port|protocol
For example, if we want to monitor the ip-address 172.83.44.3 at port 80 and protocol 6 (= TCP), we have to write 172.83.44.3/32:80|6 in one line of the file. In this case, netmask is 32, i. e. no subnet will be considered, but the ip-address itself.
With the use of wildcards for ports or protocols, the user can tell the module, that he isnt interested in these two. Wildcards are -1 for both of them. The Wildcard for ip addresses is the value 0 for the netmask. So, 172.83.44.3/32:-1|-1 means, that we are interested in ip-address 172.83.44.3 in general, no matter which port or protocol is used. Every endpoint, containing 172.83.44.3 as ip-address will thus be tested. 172.83.44.3/0:80|-1 would mean, that you are only interested in endpoints containing port 80, no matter which ip-address or protocoll they use.
"all" can also be used as value for this parameter, meaning that all endpoints will be considered. This is the default value for this parameter, if it is omitted.
Please note, that this parameter is for filtering endpoints, not for aggregating them! To aggregate endpoints, use the endpoint_key and netmask parameters! And be careful to provide endpoints which correspond to your selected endpoint_key and netmask! Bad example: You choose "port" as endpoint_key and define 172.83.44.3/32:80|6 as one of the endpoints you are interested in. This endpoint never occurs because all endpoints look like 0.0.0.0:port|0. Good Example: You choose "port" as endpoint_key and define 0.0.0.0/32:80|0 as one of the endpoints you are interested in. You could also choose 172.83.44.3/0:80|-1. If you defined a global netmask via the netmask parameter, you should also be careful with your filters. Bad example: You choose 16 as the global netmask and add a filter like 172.83.44.3/32:80|6 --> the last 16 bits of every ip will be set to 0 by the global netmask, so your filter never matches. Good example: Choose 172.83.44.3/16:80|6 or 172.83.0.0/32:80|6 instead.
!!! NOTE: In ONLINE MODE, this parameter will always be used. In OFFLINE MODE, the x_frequently_endpoints parameter will be checked first and if defined, used instead !!!


<x_frequently_endpoints>: // FOR FILTERING

This parameter sets the number of endpoints to be considered for the offline tests. The endpoints are sorted by number of occurances in all the test intervals. That means, if you choose 10 as value for this parameter, tests will be performed only for the 10 most frequently appearing endpoints of your data. This parameter was mainly introduced to reduce the number of output files generated by the tests and thus to keep track of the results. But it is also very useful to be able to test only the most frequently appearing endpoints rather than user defned endpoints. If omitted, DEFAULT_x_frequent_endpoints will be used.
!!! NOTE: This parameter is only usable in OFFLINE MODE. If defined, it will be used instead of the endpoints_to_monitor parameter !!!


<offline_file>:

This parameter defines the file, to which data shall be written in ONLINE MODE or from which data is read in OFFLINE MODE. You HAVE TO provide a file, if you are about to run the module in OFFLINE MODE (thats the basic idea of the offline mode ...). The module will exit and remind you to do so, if you didnt. In ONLINE MODE, a default file will be used to write data to, if parameter is omitted. If you explicitly dont want to have the data written into a file in ONLINE MODE, use "none" as value for this parameter.


<output_dir>:

If you define this parameter, output files will be generated for the tests of this module. They will be stored in the directory you specify as value for this parameter. If this parameter is omitted, no output files will be generated. If the directory already exists, no output will be generated and a warning message is printed. There are two kinds of output files: those files which hold all the values of the metrics (pca_components) of each endpoint and those files which contain all the parameters of the tests for every metric (pca_component) and every endpoint. Now, you should understand our intention to introduce the filter and x_frequently_endpoints parameters ;)


<stat_test_frequency>:

This parameter tells the module to perform a statistical test every X
"test-runs" (see <alarm_time>), i.e. every
<stat_test_frequency>*<alarm_time> seconds. If not provided, a warning
message will be issued, and default value will be used
(X = DEFAULT_stat_test_frequency).


<report_only_first_attack>:

If set to true, the module will only display the first "ATTACK!" message
in a series of several results hinting at an attack. This helps keep the
log file more readable. Moreover, as an attack usually covers many
Stat::test()-runs, only the first alarm message is relevant.
This parameter is a case-insensitive boolean that will be set to true
if no value is provided (after an information message).


<pause_update_when_attack>:

For the WKP-Tests, this parameter is for "pausing" the update function when one ore more attacks occurs, i.e. it updates only the "new" sample, so that the anomaly is not
assumed to be normal traffic when the corresponding sample values are sent
into the "old" sample. This avoids detecting another anomaly when the
current anomaly ceases and traffic returns to normal values.
This parameter can have values 0, 1 or 2 and is set to 0 (after a warning
message) if omitted.
0 - no pausing at all
1 - pause, if (at least) one of the wkp-tests detects an attack
2 - pause, if all three tests detect an attack
3 - same as 0
For the cusum-test, the values are related to the metrics. That means, updates for alpha will only be paused, if
1 - at least one metric yielded an alarm
2 - all metrics yielded an alarm
3 - pause update for every metric individually.



-------------------
<cusum-test-params>
-------------------

<cusum_test>:

This parameter tells the module whether the cusum-test shall be performed or not.
Possible values: true (perform it) or false (dont perform it). If not set, true
assumed (after some warning message).


<amplitude_percentage>:

This parameter represents the relative increase in the mean after an attack occured. As soon as the test-statistic g does make this jump, an alarm will be raised (note: with the next parameter, you can define how often this jump needs to be taken until an alarm is thrown). It will be multiplied with the current mean of normal behaviour (alpha) to help calculating the threshold N at which an alarm shall be raised. Possible values are floating point values and if omitted, DEFAULT_amplitude_percentage will be used as default value.


<repetition_factor>:

This parameter defines, how often the test-statistic g may make the jump mentioned above consecutively until an alarm is raised. Thus, this parameter is somehow related to the amplitude_percentage. But whilst the amplitude_percentage represents the "sensitivity" of the increase of the test-statistic g (that means, how large the values of X must be to make g positive), this parameter helps you in controlling your false alarm rate by making the test less sensitive to outliers and by this means more robust. It expects integer values and may not be 0 or negative. A good choice would be 2 or 3. If omitted, DEFAULT_repetition_factor will be used as default;


<learning_phase_for_alpha>:

This parameter expects an positive integer value which indicates the number of samples to be collected for each endpoint, before the initial mean value alpha is calculated. Only after alpha was calculated, the cusum-test can start for that endpoint. The higher the value for this parameter, the more precise alpha will be. If omitted, DEFAULT_learning_phase_for_alpha will be assumed. This value may not be negative or zero!


<smoothing_constant>:

This parameter expects floating point values greater than zero and represents the smoothing constant for the exponentially weighted moving average function used to smooth the mean value alpha. Alpha will be calculated via
alpha(n) = alpha(n-1) * (1 - smoothing_constant) + X(n-1) * smoothing_constant
That means: the greater the value of the smoothing constant, the more weight will be given to the last value of the metric and the less weight will be given to the last mean alpha. It is wise to use small values for the smoothing constant so that the last observed value has only little impact on the new mean. If omitted, DEFAULT_smoothing_constant will be used as default value for.



-----------------
<wkp-test-params>
-----------------

<wilcoxon_test>, <kolmogorov_test>, <pearson_chi-square_test>:

These parameters tell the module which statistical tests to
perform. They are case-insensitive booleans that will all be assumed
to true (after some warning messages) if not given.


<sample_old_size>, <sample_new_size>:

These parameters are the length of the two sample vectors used by the
statistical tests. The former is the "profile", the "past" of the
monitored network, to which the later, the "present" of the network,
is compared. If not provided, default values are used
(DEFAULT_sample_old_size und DEFAULT_sample_new_size).


<wmw_two_sided>:

This case-insensitive boolean tells if Wilcoxon-Mann-Whitney statistical
test should be performed assuming the alternative hypothesis is
one-sided or two-sided. Default is false, and will be assumed
after some warning message, if the user doesn't provide it.
This parameter is active only if Wilcoxon-Mann-Whitney test is enabled;
Kolmogorov-Smirnov and Pearson chi-square tests are one-sided only
(see the comments in their source files, and the STATS file if you need
to revive some memories about one-sided and two-sided tests).


<significance_level>:

If this parameter is provided, the module will display an "ATTACK!"
message everytime the statistical tests yield a p-value hinting at the
possibility of an attack, to significance level <significance_level>.
Usually, we use 0.05. If a value smaller than 0 or greater than 1 is
given, the module will display an error message and exit. If no value
is given, the module will not display any warning message, nor any
"ATTACK!" message.



II - Some words about testing
=============================

There are testing capacities implemented in the module. This enables the user to
try finding appropriate test-parameters and settings. The testing mode ist called
OFFLINE MODE and can be enabled by editing the cmake-configuration. Type

module-home$ make edit_cache

in the module's directory and toggle the OFFLINE option to ON.
Hit "c" and then "q" to reconfigure and save the changes. Now, call

module-home$ make

to recompile the module. The module will now use another InputPolicy, called
OfflineInputPolicy, which enables the Stat class to read the data from a file
and is now indepedent from the alarm_time parameter, meaning that the tests
are performed as quick as your machine can run the process rather than waiting
the alarm_time seconds. Datasets with a length in hours can be tested in just a
few seconds/minutes. Another advantage of the OfflineInputPolicy is that the
module can now directly be started without the collector. Do so by typing

module-home$ ./wkp-module wkp-module-offline.xml

As you can see, you need to specify the configuration file, as the module cant
no longer get the path to the file from the collector's preferences. So be sure
that you put the file in the module's directory. Take also care, that there
HAS TO BE an offlinefile with the data saved to it from a previous ONLINE-MODE-Run
of the module. Use the offline_file parameter to specify it.

In OFFLINE MODE, there are two ways of filtering endpoints to be monitored from
the rest of the data: Either you define a file with these endpoints inside and use
the endpoints_to_monitor parameter to specify that file or you can use the
x_frequently_endpoints parameter to say the module to only consider the x most
frequently appearing endpoints of the data. Feel free to use neither of the both
and have the tests run on all the endpoints out there (in the dataset).

If you want all the metric-values and different test-parameters to be printed into
some output files, use the output_dir parameter. If you specify a directory with this
parameter, the module will put all these files in there (if possible). So you can have
a look on the values and params later or you can use these files to visualize some
of the data. By the way, these output files can also be generated in ONLINE MODE.

Last but not least, i summarize all the parameters involved in either ONLINE
or OFFLINE MODE or both:

parameter                         ONLINE                     OFFLINE
---------------------------------------------------------------------------
<alarm_time>                 used as defined             ignored and set to 0
<offline_file>               used as defined             obligatory!
<x_frequently_endpoints>     ignored                     used as defined (filter; higher priority as endpoints_to_monitor)
<endpoints_to_monitor>       used as defined (filter)    used as defined (only if x_frequently_endpoints isnt defined!)

All other parameters are the same for ONLINE and OFFLINE MODE.

Special attention needs also to be given to the Aggregation/Filtering capacities of the
module in both, ONLINE and OFFLINE MODE. Be sure that you define filters that correspond
to the (aggregated) form of the endpoints! See information for endpoints_to_monitor
parameter in section I.

Please remember: The module's configuration file and the offlinefile have to be in
the module's directory instead of the working_dir!

If you have tested enough and want to run the module in ONLINE MODE now or again,
redo the above steps but set the OFFLINE option to OFF again.




III - How does it work? (horribly obsolete!!!)
======================

Wilcoxon-Mann-Whitney, Kolmogorov-Smirnov and Pearson chi-square tests
are somewhat similar: they are both performed on two sample vectors,
one representing the "old" network activity profile, and the other the
"recent" network activity profile. They test the null hypothesis H0
that both samples are drawn from the same distribution against the
alternative H1 that they are not. H0 means that network traffic has
not changed between "old" and "recent", which is the case under normal
operation, whereas H1 hints at an anomaly, perhaps an attack.

Writing three different modules would mean writing the same code,
except when it comes to the mathematical tests. Everything else (the
data storage class, the sample learning-and-update function, the main
parts of the detection base) is the same. Thus we write only one
detection module, able to perform all three tests.

Here is a short description of every source file:

stat-store.h/cpp:
-----------------

Defines and implements the data storage class, StatStore. Information
received from the monitor is stored into this storage class into a
C++ "map" container whose key field is an EndPoint and value field
is a structure containing the numbers of packets, bytes and records in both
"IN" and "OUT" directions (a map is a kind of table whose keys are
always sorted):

+-----+     +--------------+--------------+------------+------------+--------------+--------------+
| EP1 |-----| #packets <-- | #packets --> | #bytes <-- | #bytes --> | #records <-- | #records --> |
+-----+     +--------------+--------------+------------+------------+--------------+--------------+
| EP2 |-----| #packets <-- | #packets --> | #bytes <-- | #bytes --> | #records <-- | #records --> |
+-----+     +--------------+--------------+------------+------------+--------------+--------------+
  ...             ...            ...           ...          ...

This map container is destroyed with the StatStore object when a
"test-run" occurs ("alarm time" reached: a "test-run" is the time when
the module gathers information from the data-store, updates its own
samples information, and possibly, but not systematically, runs
statistical tests). Before destroying the map, the module will grab
the relevant information (see stat-main.h/cpp) to update its own
samples information. At the beginning of the next test-run, an empty
map will be created with a new Stat-Store object, and information will
be gathered from the collector until the next "test-run".

As StatStore object are destroyed and renewed each "alarm time"
seconds, we had to make extensive use of static members to have
StatStore objects share common, fixed and unchanging information of
crucial importance to its operation, such as the IP addresses to
monitor, the protocols to monitor, the ports to monitor, the subnet
mask to apply to all encountered IP addresses... This information
reflects the user's preferences defined in the XML configuration
file. It is extracted from the XML file by the Stat::init() function
(see stat-main.h/cpp) and passed to the Stat-Store class by the
truchment of static and public "setters" related to these static
members.


stat-main.h/cpp:
----------------

Defines and implements a class, Stat, derived from DetectionBase,
which contains the test() function, the samples, all the parameters
defined in the XML configuration file, as well as the means to extract
them from the configuration file and to warn the user if they are not
consistent ("user-friendly" interface). In a word: almost everything.

Defines and implements another class, DirectedIpAddress, derived from
IpAddress, which will be used as key in the C++ "map" container
storing "old" and "recent" sample information. Samples, as well as
tests, are indeed relative to a particular monitored IP address, in a
particular direction ("OUT" traffic or "IN" traffic, i.e. IP address
seen as a Source address or as a Destination address).

The core of the Stat class is this sample container. It is implemented
as a C++ "map" container so that it will always be sorted; the key is
a DirectedIpAddress and the value is a structure containing two "list"
containers (to enable easy removal of elements at the beginning or the
end, which will be useful when "updating" samples and which was not
provided by "vector" containers):

+---------+     +---------------------+----------------------------+
| IP1 --> |-----| "old" sample (past) | "new" sample (recent past) |
+---------+     +---------------------+----------------------------+
| IP1 <-- |-----| "old" sample (past) | "new" sample (recent past) |
+---------+     +---------------------+----------------------------+
| IP2 --> |-----| "old" sample (past) | "new" sample (recent past) |
+---------+     +---------------------+----------------------------+
| IP2 <-- |-----| "old" sample (past) | "new" sample (recent past) |
+---------+     +---------------------+----------------------------+
    ...                  ...                       ...

The sample lists are lists of integers, representing, depending on the
user's choice:

- the number of packets;
- the number of bytes;
- the number of bytes per packet;
- the number of outcoming packets minus the number of incoming packets;
- the number of outcoming bytes minus the number of incoming bytes.
- the number of packets at time t minus the number of packets at time t-1;
- the number of bytes at time t minus the number of bytes at time t-1.

The module does in fact not handle octets/packet, but rather
1000*octets/packet, the result being an integer: this trick enables to
"simulate" a float division and increase precision, without re-writing
loads of existing code.

The Stat class constructs and maintains this "table" by retrieving
information from the StatStore class at every "test-run" (see
stat-store.h/cpp). If the IP addresses found in the Stat-Store class
are new to the Stat class, a new "line" is inserted into this table
(in fact, two new lines, one in each direction); if not, sample
information is updated:

- by adding the new element to the "old" or "new" sample if one of
  them has not reached its maximal size (again a parameter set by the
  user); that's what we call the "learning phase":
      +---------------------+----------+
  t   | 15 65 15 58         |          |
      +---------------------+----------+
      +---------------------+----------+
  t+1 | 15 65 15 58 64      |          |
      +---------------------+----------+

- by dropping the oldest element (first from "old" sample), shifting
  the first element of the "new" sample to be the last of the "old"
  sample, and adding the new element as the last element of "new", if
  both of them have reached their maximal sizes; we call that the
  "update phase":
      +---------------------+----------+
  t   | 15 65 15 58 64 9 76 | 53 12 89 |
      +---------------------+----------+
      +---------------------+----------+
  t+1 | 65 15 58 64 9 76 53 | 12 89 27 |
      +---------------------+----------+

The other main parts of the Stat class are the "init" function and the
"test" function.

The job of the init function is to extract the user's preferences from
the XML configuration file. This is not a difficult work, but a
tedious one to program, for there is quite an important number of
parameters to extract, and every human error has to go reported (the
user can forget to provide parameters or provide inconsistent or
unsupported parameters, and so on). For the sake of readability, the
init function was divided into init_<task> functions, where <task> is
usually the name of the parameter being processed.

It should be noted that it is the init function that sets the static
members of the StatStore class (monitored IPs, protocols, ports, etc:
see stat-store.h/cpp) according to the user's preferences.

The test function manages the sample information as explained before,
and, from time to time (here again depending from the user's
preferences), run statistical tests. Therefore is it written in two
parts:

- the first part retrieves information from the StatStore class (call
  to the Stat::extract_data function) and updates sample information
  in the learning and update phases (call to the Stat::update
  function);

- the second part tests if it is time to run statistical tests, and do
  so if it is (call to Stat::stat_test). It is very easy to add any
  other statistical test operating on an "old" and a "new" sample by
  modifying only this Stat::stat_test function.

The word "test-run" comes from this function, which is run every
"alarm time" seconds. One should beware not to get confused with the
"test" and the "stat_test" functions.

main.cpp:
---------

Defines a Stat object and starts the related module.

wmw-test.h/cpp:
---------------

Wilcoxon-Mann-Whitney statistical rank test.

ks-test.h/cpp:
--------------

Kolmogorov-Smirnov statistical test.

pcs-test.h/cpp:
-----------------

Pearson chi-square statistical test.

shared.h/cpp:
-------------

The EndPoint and FilterEndPoint classes and all their methods. the Info struct
and some output operators.



IV - Known bugs and limitations
================================

Two limitations:

- no mechanism of size limitation is provided for the output file (logfile),
  and its size keeps growing ever and ever (fast if output verbosity is set
  to some high value) --> don't run the module for weeks, or find and implement
  some size limitation mechanism.

- the wkp-tests use /dev/null to send their output when logfile_output_verbosity is not
  the maximal verbosity --> don't run the module on non-Unix machines,
  or find and code a way to emulate a trash bin.

No known bug yet, but there are probably loads of them.



--

For any request, comment, bug report, wedding proposal, etc,
feel free to email romain(.)michalec(at)ensta(.)org

2006/09/28
